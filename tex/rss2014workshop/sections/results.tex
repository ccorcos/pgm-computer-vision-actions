\section{Experimental Results}



    We evaluated the proposed approach on a dataset consisting of $N = 4$ books in $I = 4$ poses. We used two pairs of books which are ambiguous from the back and unambigious from the front. \figref{fig:object_dataset} shows the covers of all the books used for the experiment. All object poses are presented in \figref{fig:pose_dataset}. 
    
$M = 654$ unique features were extracted from a set of ideal images of each object-pose pair. We recorded $100$ training sample for each object-pose pair to learn the likelihood distribution $\prob{f|o,p}$. For the ambiguous case (the book looks the same from the cover down pose) we use the same training images to amplify the ambiguity of the object recognition algorithm.

    Our experimental setup consists of a Microsoft Kinect sensor which we use as a RGB camera and one of the books. All the actions are executed by a human and we assume that there is no noise in the action execution. 

    \subsection{Object Recognition}
    
	In order to evaluate the object recognition model, we trained the model on $80$ samples and held out $20$ samples for cross validation. The average prediction accuracy results for the unambiguous cases are presented in \tabref{tab:accuracy}. It is worth emphasizing that we did not include the ambiguous poses in the cross validation results because these ambiguous cases were designed to cause static object recognition to fail. Please note that all classification errors are due to confusion about whether the spine was visible or not.    
    
 
        
        \begin{table}[h]
                \centering
                \begin{tabular}{|c|c|}
                \hline
                Average Training Accuracy & Average Cross Validation Accuracy \\
                \hline
                99.67\% & 93.75\% \\
                \hline
                \end{tabular}
                \caption{Average accuracy results for object recognition part of the model.}
                \label{tab:accuracy}
		\end{table}

    \subsection{Action Selection}
	Our action selection experiment is represented by the decision tree in \figref{fig:tree}.    
	First, the ambiguous back of the book pose is observed as shown in \figref{fig:posteriors} (top-left). As expected, the posterior probabilities is split between the two ambiguous books shown in \figref{fig:posteriors} (top-right).    

 Of the four actions, staying and rotating result in similarly ambiguous poses resulting in an expected entropy of $0.7$. Flipping the book over, with and without rotating, lead to similarly unambiguous poses with an expected entropy of $0$. Because both actions lead to comparably unambiguous poses, the best action can be decided based on some criteria such as minimum energy. After flipping the book, the robot observes the cover of the book (\figref{fig:posteriors} bottom-left)and knows precisely what book it is observing (\figref{fig:posteriors} bottom-right).

        The expected entropy is calculated again for each action and they are all $0$, which means that there is no action that will provide the reduction of the expected entropy - the algorithm has converged. %Once the object recognition algorithm has converged to 100\% certainty, the updated posterior will always remain the same as the previous posterior.
    
    
        %The experiment is constructed as follows. We expose an object to the Kinect sensor and obtain the recognition result. In the next step we employ the action selection algorithm to choose the best action and we execute it.~\figref{fig:tree} presents a decision tree that is visualizes the best action actions chosen by the algorithm and the respective observations. In order to measure if the action improved the previous recognition we compute the posterior probability of the object, which is shown together with the respective observation in~\figref{fig:posteriors}. We execute actions until the algorithm converges which means that the action selection algorithm returns says that all the actions do not provide a reduction of expected entropy.

    %\subsection{Discussion}
    % describe the behaviour of posteriors
    % explain why it chose this action
    % explain why we took this exagurated example of the books with stickers
    % show encyclopedia example
    % talk a bit about continous pose    
        
\begin{figure}
\centering
    		\includegraphics[width = 0.2\columnwidth]{pics/math_cover1_ok.jpg}
    		\includegraphics[width = 0.2\columnwidth]{pics/math_cover2_ok.jpg}
    		\includegraphics[width = 0.2\columnwidth]{pics/first_cover1.jpg}
    		\includegraphics[width = 0.2\columnwidth]{pics/first_cover2.jpg}
    		\caption{Books from the cover side used for the experiment. Two first books and two last books look the same from the back side.}
	\label{fig:object_dataset}
    \end{figure}     
    
    
     \begin{figure}
     \centering
    		\includegraphics[width = 0.2\columnwidth]{pics/math_cover1.jpg}
    		\includegraphics[width = 0.2\columnwidth]{pics/math_cover1_rot.jpg}
    		\includegraphics[width = 0.2\columnwidth]{pics/math_down.jpg}
    		\includegraphics[width = 0.2\columnwidth]{pics/math_down_rot.jpg}
    		\caption{All the poses that are used for object-pose recognition: cover up with visible spine, cover up with no visible spine, cover down with visible spine and cover down with no visible spine.}
	\label{fig:pose_dataset}
    \end{figure}   
        
        
    \begin{figure}
    \centering
    		\includegraphics[width = 0.7\columnwidth]{pics/tree_small.png}
    	\caption{Decision tree based on the action selection algorithm. Each node in the tree represents the expected entropy of the posterior probability for a given action. Colored nodes indicate the choice of the action that results in the minimum expected entropy. Directed edges correspond to all possible actions.  }
    	    	\label{fig:tree}
    \end{figure}
    
    \begin{figure}
    	    \raisebox{0.5\height}{\includegraphics[width = 0.39\columnwidth]{pics/obs1.jpg}}
    		\includegraphics[width = 0.6\columnwidth]{pics/experimentObs1.png}\\
 		\raisebox{0.5\height}{\includegraphics[width = 0.39\columnwidth]{pics/obs2.jpg}}
    		\includegraphics[width = 0.6\columnwidth]{pics/experimentObs2.png}

	\caption{ Top-left: observed image for the first observation before the action was taken. Top-right: Posterior probability of object and pose after the first observation.	Bottom-left: observed image for the second observation after the action was taken. Bottom-right: Posterior probability of object and pose after the second observation..}
    	\label{fig:posteriors}
    \end{figure}
    
    
    
    
        
        


        %*explain our setup
        % we have a kinect, 4 object, 4 poses each, 50 sift features per object-pose pair
        % 100 training images per object-pose pair
        % we move the object with our hands for now
        %  

        %*assumptions
        % perfect actions (no noise)
        % discrete poses 
        % we see one object at a time
        % we see a known object

        %*explain experiment
        % we show the object to the kinect and recognize it based on our recognition module
        % results for all objects and poses are in the table
        % we figure out the best action (in the table)
        % we check recognition after the action has been taken (in the table)


%        \begin{itemize}
%        \item explain our dataset
%        \item show pictures of objects
%        \item show cross validation results for our training
%        \item explain the setup
%        \item show real data results for object recognition (table)
%        \item discuss the results, emphasize that the action selection works
%        \end{itemize}